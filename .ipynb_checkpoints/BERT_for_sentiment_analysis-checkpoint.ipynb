{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcpi1xj7CRYJ"
   },
   "source": [
    "## BERT for sentiment analysis using Amazon Reviews data\n",
    "\n",
    "Cite code source: https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1oL1UJRr57d"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cebDnH2rruvN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hhzq9VVer9tD"
   },
   "outputs": [],
   "source": [
    "pajama_1 = pd.read_csv(\"Amazon_reviews_Pajama_1.csv\", index_col= 0)\n",
    "pajama_2 = pd.read_csv(\"Amazon_reviews_Pajama_2.csv\", index_col= 0)\n",
    "Tshirts_1 = pd.read_csv(\"Amazon_reviews_Tshirts_1.csv\", index_col= 0)\n",
    "Tshirts_2 = pd.read_csv(\"Amazon_reviews_Tshirts_2.csv\", index_col= 0)\n",
    "jeans_1 = pd.read_csv(\"Amazon_reviews_jeans_1.csv\", index_col= 0)\n",
    "jeans_3 = pd.read_csv(\"Amazon_reviews_jeans_3.csv\", index_col= 0)\n",
    "jeans_4 = pd.read_csv(\"Amazon_reviews_jeans_4.csv\", index_col= 0)\n",
    "shirts_2 = pd.read_csv(\"Amazon_reviews_shirts_2.csv\", index_col= 0)\n",
    "shoes_1 = pd.read_csv(\"Amazon_reviews_shoes_1.csv\", index_col= 0)\n",
    "shoes_2 = pd.read_csv(\"Amazon_reviews_shoes_2.csv\", index_col= 0)\n",
    "sweaters_1 = pd.read_csv(\"Amazon_reviews_sweaters_1.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BrPfvcBQsJ3j"
   },
   "outputs": [],
   "source": [
    "pajama = pd.concat([pajama_1,pajama_2], ignore_index=True)\n",
    "Tshirts = pd.concat([Tshirts_1,Tshirts_2], ignore_index=True)\n",
    "jeans = pd.concat([jeans_1,jeans_3,jeans_4], ignore_index=True)\n",
    "shoes = pd.concat([shoes_1,shoes_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VuzGMnjpsMSO"
   },
   "outputs": [],
   "source": [
    "all_product = pd.concat([pajama, Tshirts, jeans, shoes, shirts_2, sweaters_1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "AktvFS_JsN3S",
    "outputId": "166c5442-e085-4b0b-cd6c-514c763eae6f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>starRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n  You can't imagine such bad quality on na...</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n  The cloth is okay, but the stitches star...</td>\n",
       "      <td>2.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n  These are very comfortable to wear. Once...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n  If you put this into the washing machine...</td>\n",
       "      <td>2.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  Very poor quality......got damaged produ...</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6659</th>\n",
       "      <td>\\n\\n  Very good fit\\n\\n</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6660</th>\n",
       "      <td>\\n\\n  üòèüòè\\n\\n</td>\n",
       "      <td>2.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>\\n\\n  Good product i like it very much\\n\\n</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>\\n\\n  worth for moneyperfect size as expectedg...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6663</th>\n",
       "      <td>\\n\\n  Just ok for price paid\\n\\n</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6664 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews          starRating\n",
       "0     \\n\\n  You can't imagine such bad quality on na...  1.0 out of 5 stars\n",
       "1     \\n\\n  The cloth is okay, but the stitches star...  2.0 out of 5 stars\n",
       "2     \\n\\n  These are very comfortable to wear. Once...  5.0 out of 5 stars\n",
       "3     \\n\\n  If you put this into the washing machine...  2.0 out of 5 stars\n",
       "4     \\n\\n  Very poor quality......got damaged produ...  1.0 out of 5 stars\n",
       "...                                                 ...                 ...\n",
       "6659                            \\n\\n  Very good fit\\n\\n  4.0 out of 5 stars\n",
       "6660                                       \\n\\n  üòèüòè\\n\\n  2.0 out of 5 stars\n",
       "6661         \\n\\n  Good product i like it very much\\n\\n  5.0 out of 5 stars\n",
       "6662  \\n\\n  worth for moneyperfect size as expectedg...  4.0 out of 5 stars\n",
       "6663                   \\n\\n  Just ok for price paid\\n\\n  3.0 out of 5 stars\n",
       "\n",
       "[6664 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzHWI4_vsQiy"
   },
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bDwWVBRusO_y"
   },
   "outputs": [],
   "source": [
    "all_product_new = all_product.replace({'1.0 out of 5 stars': 1, \n",
    "                                   '2.0 out of 5 stars': 2,\n",
    "                                   '3.0 out of 5 stars': 3,\n",
    "                                   '4.0 out of 5 stars': 4,\n",
    "                                   '5.0 out of 5 stars': 5})\n",
    "all_product_new['reviews'] = all_product_new['reviews'].map(lambda x: str(x)[4:-2])\n",
    "\n",
    "# convert ratings to sentiment\n",
    "# ratings <= 2: 0\n",
    "# ratings = 3: 1\n",
    "# ratings >= 4: 3\n",
    "def covert_to_sentiment(rating):\n",
    "      if rating <= 2:\n",
    "        return 0\n",
    "      elif rating == 3:\n",
    "        return 1\n",
    "      else:\n",
    "        return 2\n",
    "\n",
    "all_product_new['sentiment'] = all_product_new.starRating.apply(covert_to_sentiment)\n",
    "\n",
    "all_product_new[['reviews', 'sentiment']].to_csv('preprocessed_data.csv')\n",
    "\n",
    "df = all_product_new[['reviews', 'sentiment']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9c5zmuhsp-7"
   },
   "source": [
    "## BERT\n",
    "\n",
    "Use transformers version 3.5.1 and torch version 1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgx8cjHSscHI",
    "outputId": "579f2415-dfa2-4b1c-986a-63953e6ce727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 4.2 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 34.3 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9 MB 39.2 MB/s \n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 43.0 MB/s \n",
      "\u001b[?25hCollecting torch==1.4.0\n",
      "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 753.4 MB 1.5 kB/s \n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.0+cu111\n",
      "    Uninstalling torch-1.10.0+cu111:\n",
      "      Successfully uninstalled torch-1.10.0+cu111\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
      "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq transformers==3.5.1\n",
    "!pip install torch==1.4.0\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c31f08a2e1b141d8ad890375934036a6",
      "3151a61c5f824d1c987869a39a8d3d55",
      "5e245282cee34bb09905c93f84c45cbb",
      "877c8a631bd54595a66165a7c31137db",
      "2dddf1830be54c5aaf4680f0d8edd3b8",
      "97ebdf19fdc0458aaa20efa0f8741837",
      "ccc45c81f2e94ddcb8b03a51b06ef8dc",
      "62831f1fe5a04cfb980eb288bdbd198f",
      "207618094cc944c998f49b741f7d46a5",
      "7fc8c787e07d4823a2767e9797f6917f",
      "474e8e10b77e4fee8bcc7daafdc7f09f"
     ]
    },
    "id": "IA0XKwjms33b",
    "outputId": "92b1fbe9-f4c5-4865-ec97-ab0d41867316"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31f08a2e1b141d8ad890375934036a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_tranined_model = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pre_tranined_model)\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "B14LqTYQs-3p",
    "outputId": "3966c85f-c7d2-4ab5-b5ba-88fd30fdb756"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Token count')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set MAX_LEN\n",
    "token_lens = []\n",
    "for review in df.reviews:\n",
    "    review = \" \".join([word for word in review.split() if word.lower() not in stopwords])\n",
    "    tokens = tokenizer.encode(review, max_length=512)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256])\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "U94_WmprtPbG"
   },
   "outputs": [],
   "source": [
    "# from plot, choose MAX_LEN\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6Fsu9XoxtTjM"
   },
   "outputs": [],
   "source": [
    "class AmazonReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          review,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5Fsaw_fBvMaX"
   },
   "outputs": [],
   "source": [
    "# train, val, test set split = 70/15/15\n",
    "df_train, df_test = train_test_split(\n",
    "  df,\n",
    "  test_size=0.3,\n",
    "  random_state= 42\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "  df_test,\n",
    "  test_size=0.5,\n",
    "  random_state= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAdoaujRvZ-G",
    "outputId": "90b7e23d-93dc-4ff3-937a-dd922483870d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4664, 2), (1000, 2), (1000, 2))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Eis15jd8vcff"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = AmazonReviewDataset(\n",
    "        reviews = df.reviews.to_numpy(),\n",
    "        targets = df.sentiment.to_numpy(),\n",
    "        tokenizer = tokenizer,\n",
    "        max_len = max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OhMFqo7NvsYH"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MbS_IYTTvtde"
   },
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(pre_tranined_model, return_dict=False)\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pre_tranined_model)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          return_dict=False\n",
    "        )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_ysrN7SewGBa"
   },
   "outputs": [],
   "source": [
    "class_names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "moSkhm4WwV-Q"
   },
   "outputs": [],
   "source": [
    "# suggested hyperparameters tuning\n",
    "# Batch size: 16, 32\n",
    "# Learnning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "# Number of epochs: 2,3,4\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VWhd7KviwyV0"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer,\n",
    "                device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "anBC5UQhw3uA"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "              input_ids=input_ids,\n",
    "              attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqCwhkkWw6Nq",
    "outputId": "5fe6fd54-b5e8-46d7-d8d7-b954a194e8d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4891267508166294 accuracy 0.8336192109777015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.4637648503458689 accuracy 0.8190000000000001\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3705991371470976 accuracy 0.8737135506003431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.5137905362579558 accuracy 0.8190000000000001\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.285525225141175 accuracy 0.9114493996569468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.6000016173200001 accuracy 0.8160000000000001\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.22959627463699203 accuracy 0.9365351629502573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.6898599657274428 accuracy 0.8180000000000001\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.17764404345916152 accuracy 0.9521869639794168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.8076688565668606 accuracy 0.8240000000000001\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.15716928981040437 accuracy 0.9596912521440824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.8571551277760475 accuracy 0.8240000000000001\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.1370686254119628 accuracy 0.9631217838765008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.8987325651068536 accuracy 0.8270000000000001\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.1281235138949466 accuracy 0.9661234991423671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.9171634654677103 accuracy 0.8290000000000001\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.11345827407267405 accuracy 0.9680531732418525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.9470639699508273 accuracy 0.8260000000000001\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.11187731785291474 accuracy 0.9689108061749571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.9568681118507234 accuracy 0.8230000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "      model,\n",
    "      train_data_loader,\n",
    "      loss_fn,\n",
    "      optimizer,\n",
    "      device,\n",
    "      scheduler,\n",
    "      len(df_train)\n",
    "    )\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "      )\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBi3A5B5BmSN"
   },
   "source": [
    "Output\n",
    "\n",
    "Loss: 0.957\n",
    "\n",
    "Accuracy: 0.823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "9DX8X5p8xDiP",
    "outputId": "5d2314af-d3ca-4bf8-e555-69dc8f863f46"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hV9Z3v8fc3N0ICgVwAlSBERcEbIteqtbaWGawttCqlVuvoo2XqqahT2xmOp2Ottc9xbMextLRTbG21tSpDxxY7tM7I4KEdLyVoRURUFJQISgjXALns7O/5Y61sdkISNpCVTbI+r8c8e11+e+1vtuH3WXtdftvcHRERia+cbBcgIiLZpSAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxBIn2Zmvzezv+nutodZw0VmVtPF+n81s3/s7tcVyZTpPgI51phZfdpsEdAItITzf+vuj/R8VUfOzC4CfunulUe5nY3ADe7+dHfUJdIqL9sFiLTn7gNap7vq/Mwsz90TPVlbb6X3SrqiQ0PSa7QeYjGzfzCz94GfmVmpmf3OzGrNbEc4XZn2nGfM7IZw+loz+5OZfTdsu8HMLjnCtlVmtsLM9pjZ02a2wMx+eYj6bzOzrWa2xcyuS1v+czO7O5yuCH+HnWa23cz+aGY5ZvYL4ETgSTOrN7O/D9vPMLNXw/bPmNnYtO1uDN+r1cBeM/uamf26XU3zzex7R/L/Q/oOBYH0NscBZcBIYA7B3/DPwvkTgf3AD7p4/hTgdaACuBf4qZnZEbT9FfBnoBy4E/hCBnUPAoYD1wMLzKy0g3a3ATXAEGAYcDvg7v4F4F3gU+4+wN3vNbNTgUeBW8P2SwmCoiBte1cClwKDgV8C081sMASfEoDPAQ8fonbp4xQE0tskgW+4e6O773f3Onf/tbvvc/c9wLeBj3Tx/Hfc/QF3bwEeAo4n6HAzbmtmJwKTgDvcvcnd/wQsOUTdzcBd7t7s7kuBeuC0TtodD4wM2/7ROz+RNxv4D3f/L3dvBr4L9AfOS2sz3903he/VFmAFMCtcNx3Y5u6rDlG79HEKAultat29oXXGzIrM7Mdm9o6Z7Sbo6AabWW4nz3+/dcLd94WTAw6z7QnA9rRlAJsOUXddu2P0+zp53e8A64H/NLO3zWxeF9s8AXgnrcZkWMfwLup6CLg6nL4a+MUh6pYYUBBIb9N+7/g2gj3rKe5eAlwYLu/scE932AKUmVlR2rIR3bFhd9/j7re5+0nADOArZnZx6+p2zTcTHBIDIDxsNQJ4L32T7Z7zG+BsMzsT+CTQq67AkmgoCKS3G0hwXmCnmZUB34j6Bd39HaAauNPMCszsQ8CnumPbZvZJMzsl7NR3EVw2mwxXfwCclNZ8EXCpmV1sZvkEodgIPNtF7Q3AYsJzHO7+bnfULb2bgkB6u/sJjotvA54H/tBDr3sV8CGgDrgbeJygEz5ao4GnCc4hPAf80N2Xh+v+L/D18Aqhr7r76wSHd75P8Pt/iuBkctMhXuMh4Cx0WEhCuqFMpBuY2ePAOneP/BPJ0QpPdq8DjnP33dmuR7JPnwhEjoCZTTKzk8Nr/KcDMwmOvx/TzCwH+ArwmEJAWkUWBGb2YHjzzJpO1lt4M8t6M1ttZudGVYtIBI4DniE4hDMfuNHdX8pqRYdgZsXAbmAaPXAuRXqPyA4NmdmFBP9IHnb3MztY/wlgLvAJght3vufuUyIpRkREOhXZJwJ3XwFs76LJTIKQcHd/nuDa7+OjqkdERDqWzUHnhtP2ZpeacNmW9g3NbA7BcAIUFxdPGDNmTI8UKCLSV6xatWqbuw/paF2vGH3U3RcCCwEmTpzo1dXVWa5IRKR3MbN3OluXzauG3qPt3ZiVtL0jUkREekA2g2AJcE149dBUYFc4KJaIiPSgyA4NmdmjwEVAhQVf0/cNIB/A3f+VYMjcTxAMsLUPuK7jLYmISJQiCwJ3v/IQ6x34clSvLyIimekVJ4tFJF7cnZakk0gGjy3uJNOmW8LpZJI28y1JJ+kdtfNO2pHadqJdu6Q7iZbgMemOOyQdnGDaw2UOqfXujkPY1lPraG2X9DbtSX8uHmy/zbbbLrtiQiXnnVLR7e+3gkCkj0kmneZkkuYWJ9GSpKnlwHRzS5KmhJNIdj0dtG+dD56bSDrNLUkSLcH2E63bTIbrWzw13dzitCTbPye9XWfPD5cne+8YaGaQY4YRPBL8FyxLm8YOXmYGFj43fTsWrrvw1A6v/jxqCgKRiLQknX1NCfY2trC3KcG+xhbqGxPBsqYW9jYm2NuYYF/rdFMw3dziNCeSJJJJmjqZbm5xmtKmmxPJVOffEnEnmmOQl5tDfo4Fj7lGXk4OeblGfm4OeTlGbk44nWvk5+RQkJdDUeo5dsjn54XrcsPp3Bwjx4LH1I8ZOTlGbg7k5uSQa8F05+2CbRy8vQPPz8kh9ZzW5+eEbdt21mBYu8466LB7IwWBCMHH7v3NLUGnndYp1zcGHfjepnaddmPQme9rSlDf2MK+xgOde7AsQUNz8tAvHOqXl0Nxvzz65+dSkBd0jkFHmkNBON2/4MB0awdakHugA02fzs/teBvtpw9so+10XruOPOi8g+mcnN7Z2UnnFATSpyWTTt3eJrbuaWDr7kY+2N3AB7sb2bon/bGBbfVNGe9J5+UYxf3yKC7IpbhfHkXhdGlxAcUFuRT1y2NAvzyKCnIpLsgL2vbLpaggeCxufeyXR1FB0C4/VwMBS/YoCKRXSiadHfua+GB3Ix/saaC2tZNPdfCNbN3dQO2exg6PN5cVFzB0YD+GlRQy5riBDBnYj4GF+akOvqgg7MzTO+6CYL5fXmdfhyzSOykI5Jji7uzY15zaY/9gdwNbdzewdU/a3vzuBmrrG2luObiDLy3KZ+jAQoaW9GP00AqGlQSd/dCB/RhaUsiwkkKGDOhHQZ72wEVaKQikx9XVN7J+az3ra+tZv7WeLTsbUh1/7Z5GmloOPrY+qH9+qlM/aUg5w0oKGRbu0Q8t6cfQgYUMGdiPwnztrYscLgWBRMLd2bKrgfVb63lza9Dhv7W1nje37mHHvuZUu/75uQwv7c+wkn5MqSpjaLj3PqykMNXxq4MXiZaCQI5KoiXJph37ww5/T6rDX7+1nr1NLal2g4vyOWXIAKafeRwnDxnAKUMHMHrYQI4vKdRVKCJZpiCQjDQmWtiwbS9vflB/4LDOB/Vs2La3zaGcYSX9GD10ILMmjuDkoQMYPTTo9MuLC3rtNdYifZ2CQNqob0yEh3DCDn9rPeu37uHd7ftovfjGDEaUFjF66AAuOm1IqsM/eegASgrzs/sLiMhhUxDE1K79zazbsjt1wrb1Z8uuhlSb/FyjqqKY008oYca4E8IOfyAnDSnWMXuRPkRBEBO1expZuXE7f96wnRc2bGfd+7tTg171z8/llKEDmHpSOaeEh3JOGTqAE8uKdKOTSAwoCPqozTv3pzr9FzbU8XbtXiDo9CeMLOXvPn4qZ1UOYvTQAZwwqL9O2IrEmIKgD3B3Ntbt488b6nhhQ7DXX7NjPwADC/OYNKqMz04cweSqMs4aPkh7+SLShoKgF0omnTe31rfp+LfuaQSgvLiAyVVlXH9BFZOryhhzXAm52tsXkS4oCHqBREuStVt2pw71rNy4nZ3hTVnHlRTyoZPLmVJVzuSqMk4eUqzLNEXksCgIjkGNiRZeqdkVHt/fzqqN21M3Z40qL+KvTh/G5KpyplSVUVnaXx2/iBwVBcExYF9Tgpfe3Rke5qnjpXd30pgIbtI6bdhALju3kslVZUyuKmNYSWGWqxWRvkZBkAW7G5pZtXFH6oqeV2p2kUg6OQZnnDCIq6eOZHJVGZNGlVFWXJDtckWkj1MQ9IC9jQlWbtzOc2/V8exbdazZvAv34IatcZWDmXPhSUyuKmPCyFIG6s5cEelhCoIINDS38NK7O3nurW08+1Ydf9m0k0TSKcjNYfyJg7n5Y6OZclIZ40eU0r9Ad+iKSHYpCLpBoiXJ6vd2hXv826jeuIPGRJIcg7PDPf7zTq5gwkh1/CJy7FEQHIFk0nnt/d2pQz1/3rCd+sYEAGOPL+HqqSM57+RyJlWVaRA2ETnmKQgy4O68Vbs3dajnubfrUtfxnzSkmE+PP4HzTq5g6knlOrkrIr2OgqATm7bvSx3qefatutSdu8MH92fa2GGcd0o5HzqpguMG6XJOEendFAShrbsbeO7tOp5dX8ezb29j0/ZgrJ6KAf047+Ty8KeCEWW6gUtE+pbYBsGOvU28sCE4xv/sW3Ws31oPQElhHh86uZwbLjiJ804OhmVWxy8ifVlsgqC+McHKDdtTh3rWbgnG4y8qyGVyVRmfnVjJeSdXMPZ4DdImIvESmyD46R838C9Pv0FBXg4TTizlKx8/lfNOKefsysEallkkCu7gSUi2gLdAMhFOJ9OmW4LHZOJA22TiwHKA3HzIyQ8f89Lm89KW50OO/h0fqdgEwWfGD2fSqFLOHVmqr1mUQDIJif3QHP7g2a4o6AxbEmFH2QwtzcF0S3M4n768/Xxau2Si83VdbjNx4DHVSbfvsFuC965N595BW2/p2ffOctoGRqehkcHynLwunnOIQGozn9vFuq62mR98OXgPiU0QnDg4nxNLioEmaM52NXbgj0QOluqgG6B5X9hRt3/cn9aJd7CueV+753fQrqUx279pD7AMO6y0TiivAHKKwHLDDjU37GTDecsNp3MPTKfa5rR7Xm6wrM3z8g5s76C27bYLhw69zoItPdQ620aiCZJ7O2mX6OA5Pdh5WG67/1d5MO0uOOfz3f5SsQkCnv8h/Ncd2a6inQ7+kab+QWayN9NJu0y3AV18VO9oL7Cjtom0vcN2e4xtntd+u2mHCRINbTvoRMORvZ35RZBXGDzm9w9/iqCgGIqHhPPp69Me8/oFnVPW2VHuSbb/G9DORrdyD/+OOwukFjL6xNY632FYdRFeg0dG8mvFJwhGng8fvzPbVQRSH/+P8CN98/6uP9K3X3e0hzw625tLX245Xe/ZtbbNKzh4LzC/MOiQ8/of3EGnd+iddeL5/YMA0NVdEjWzIGxz84K/uz4i0iAws+nA94Bc4Cfufk+79ScCDwGDwzbz3H1pJMVUTgx+4ijZ0vHeBmTwUf1Y2EsWkShFFgRmlgssAKYBNcBKM1vi7mvTmn0dWOTuPzKz04GlwKioaoqt1r1zdBe0iBwsyt29ycB6d3/b3ZuAx4CZ7do4UBJODwI2R1iPiIh0IMogGA5sSpuvCZeluxO42sxqCD4NzO1oQ2Y2x8yqzay6trY2ilpFRGIr2weArwR+7u6VwCeAX5gdfOmGuy9094nuPnHIkCE9XqSISF8WZRC8B4xIm68Ml6W7HlgE4O7PERzEroiwJhERaSfKIFgJjDazKjMrAD4HLGnX5l3gYgAzG0sQBDr2IyLSgyILAndPADcBTwGvEVwd9KqZ3WVmM8JmtwFfNLOXgUeBa939GLjPX0QkPiK9jyC8J2Bpu2V3pE2vBc6PsgYREelatk8Wi4hIlikIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYi7SIDCz6Wb2upmtN7N5nbT5rJmtNbNXzexXUdYjIiIHy4tqw2aWCywApgE1wEozW+Lua9PajAb+N3C+u+8ws6FR1SMiIh2L8hPBZGC9u7/t7k3AY8DMdm2+CCxw9x0A7r41wnpERKQDUQbBcGBT2nxNuCzdqcCpZvY/Zva8mU3vaENmNsfMqs2sura2NqJyRUTiKdsni/OA0cBFwJXAA2Y2uH0jd1/o7hPdfeKQIUN6uEQRkb7tkEFgZp8ysyMJjPeAEWnzleGydDXAEndvdvcNwBsEwSAiIj0kkw5+NvCmmd1rZmMOY9srgdFmVmVmBcDngCXt2vyG4NMAZlZBcKjo7cN4DREROUqHDAJ3vxoYD7wF/NzMnguP2Q88xPMSwE3AU8BrwCJ3f9XM7jKzGWGzp4A6M1sLLAe+5u51R/H7iIjIYTJ3z6yhWTnwBeBWgo79FGC+u38/uvIONnHiRK+uru7JlxQR6fXMbJW7T+xoXSbnCGaY2RPAM0A+MNndLwHGAbd1Z6EiItLzMrmh7HLgX9x9RfpCd99nZtdHU5aIiPSUTILgTmBL64yZ9QeGuftGd18WVWEiItIzMrlq6N+AZNp8S7hMRET6gEyCIC8cIgKAcLogupJERKQnZRIEtWmXe2JmM4Ft0ZUkIiI9KZNzBF8CHjGzHwBGMH7QNZFWJSIiPeaQQeDubwFTzWxAOF8feVUiItJjMvo+AjO7FDgDKDQzANz9rgjrEhGRHpLJDWX/SjDe0FyCQ0OzgJER1yUiIj0kk5PF57n7NcAOd/8m8CGCweFERKQPyCQIGsLHfWZ2AtAMHB9dSSIi0pMyOUfwZPhlMd8BXgQceCDSqkREpMd0GQThF9Isc/edwK/N7HdAobvv6pHqREQkcl0eGnL3JLAgbb5RISAi0rdkco5gmZldbq3XjYqISJ+SSRD8LcEgc41mttvM9pjZ7ojrEhGRHpLJncVdfiWliIj0bocMAjO7sKPl7b+oRkREeqdMLh/9Wtp0ITAZWAV8LJKKRESkR2VyaOhT6fNmNgK4P7KKRESkR2Vysri9GmBsdxciIiLZkck5gu8T3E0MQXCcQ3CHsYiI9AGZnCOoTptOAI+6+/9EVI+IiPSwTIJgMdDg7i0AZpZrZkXuvi/a0kREpCdkdGcx0D9tvj/wdDTliIhIT8skCArTv54ynC6KriQREelJmQTBXjM7t3XGzCYA+6MrSUREelIm5whuBf7NzDYTfFXlcQRfXSkiIn1AJjeUrTSzMcBp4aLX3b052rJERKSnZPLl9V8Git19jbuvAQaY2f+KvjQREekJmZwj+GL4DWUAuPsO4IvRlSQiIj0pkyDITf9SGjPLBQqiK0lERHpSJieL/wA8bmY/Duf/Fvh9dCWJiEhPyiQI/gGYA3wpnF9NcOWQiIj0AYc8NBR+gf0LwEaC7yL4GPBaJhs3s+lm9rqZrTezeV20u9zM3MwmZla2iIh0l04/EZjZqcCV4c824HEAd/9oJhsOzyUsAKYRDF290syWuPvadu0GArcQhI2IiPSwrj4RrCPY+/+ku1/g7t8HWg5j25OB9e7+trs3AY8BMzto9y3gn4CGw9i2iIh0k66C4DJgC7DczB4ws4sJ7izO1HBgU9p8TbgsJRy6YoS7/0dXGzKzOWZWbWbVtbW1h1GCiIgcSqdB4O6/cffPAWOA5QRDTQw1sx+Z2V8d7QubWQ5wH3Dbodq6+0J3n+juE4cMGXK0Ly0iImkyOVm8191/FX53cSXwEsGVRIfyHjAibb4yXNZqIHAm8IyZbQSmAkt0wlhEpGcd1ncWu/uOcO/84gyarwRGm1mVmRUAnwOWpG1rl7tXuPsodx8FPA/McPfqjjcnIiJROJIvr8+IuyeAm4CnCC43XeTur5rZXWY2I6rXFRGRw5PJDWVHzN2XAkvbLbujk7YXRVmLiIh0LLJPBCIi0jsoCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGIu0iAws+lm9rqZrTezeR2s/4qZrTWz1Wa2zMxGRlmPiIgcLLIgMLNcYAFwCXA6cKWZnd6u2UvARHc/G1gM3BtVPSIi0rEoPxFMBta7+9vu3gQ8BsxMb+Duy919Xzj7PFAZYT0iItKBKINgOLApbb4mXNaZ64Hfd7TCzOaYWbWZVdfW1nZjiSIickycLDazq4GJwHc6Wu/uC919ortPHDJkSM8WJyLSx+VFuO33gBFp85XhsjbM7OPA/wE+4u6NEdYjIiIdiPITwUpgtJlVmVkB8DlgSXoDMxsP/BiY4e5bI6xFREQ6EVkQuHsCuAl4CngNWOTur5rZXWY2I2z2HWAA8G9m9hczW9LJ5kREJCJRHhrC3ZcCS9stuyNt+uNRvr6IiBxapEHQU5qbm6mpqaGhoSHbpcgxorCwkMrKSvLz87Ndisgxr08EQU1NDQMHDmTUqFGYWbbLkSxzd+rq6qipqaGqqirb5Ygc846Jy0ePVkNDA+Xl5QoBAcDMKC8v1ydEkQz1iSAAFALShv4eRDLXZ4JARESOjIKgG+zcuZMf/vCHR/TcT3ziE+zcubObKxIRyZyCoBt0FQSJRKLL5y5dupTBgwdHUdZRcXeSyWS2yxCRHtAnrhpK980nX2Xt5t3dus3TTyjhG586o9P18+bN46233uKcc85h2rRpXHrppfzjP/4jpaWlrFu3jjfeeINPf/rTbNq0iYaGBm655RbmzJkDwKhRo6iurqa+vp5LLrmECy64gGeffZbhw4fz29/+lv79+7d5rSeffJK7776bpqYmysvLeeSRRxg2bBj19fXMnTuX6upqzIxvfOMbXH755fzhD3/g9ttvp6WlhYqKCpYtW8add97JgAED+OpXvwrAmWeeye9+9zsA/vqv/5opU6awatUqli5dyj333MPKlSvZv38/V1xxBd/85jcBWLlyJbfccgt79+6lX79+LFu2jEsvvZT58+dzzjnnAHDBBRewYMECxo0b163/P0Ske/W5IMiGe+65hzVr1vCXv/wFgGeeeYYXX3yRNWvWpC5ffPDBBykrK2P//v1MmjSJyy+/nPLy8jbbefPNN3n00Ud54IEH+OxnP8uvf/1rrr766jZtLrjgAp5//nnMjJ/85Cfce++9/PM//zPf+ta3GDRoEK+88goAO3bsoLa2li9+8YusWLGCqqoqtm/ffsjf5c033+Shhx5i6tSpAHz729+mrKyMlpYWLr74YlavXs2YMWOYPXs2jz/+OJMmTWL37t3079+f66+/np///Ofcf//9vPHGGzQ0NCgERHqBPhcEXe2596TJkye3uYZ9/vz5PPHEEwBs2rSJN99886AgqKqqSu1NT5gwgY0bNx603ZqaGmbPns2WLVtoampKvcbTTz/NY489lmpXWlrKk08+yYUXXphqU1ZWdsi6R44cmQoBgEWLFrFw4UISiQRbtmxh7dq1mBnHH388kyZNAqCkpASAWbNm8a1vfYvvfOc7PPjgg1x77bWHfD0RyT6dI4hIcXFxavqZZ57h6aef5rnnnuPll19m/PjxHV7j3q9fv9R0bm5uh+cX5s6dy0033cQrr7zCj3/84yO6Vj4vL6/N8f/0baTXvWHDBr773e+ybNkyVq9ezaWXXtrl6xUVFTFt2jR++9vfsmjRIq666qrDrk1Eep6CoBsMHDiQPXv2dLp+165dlJaWUlRUxLp163j++eeP+LV27drF8OHB9/s89NBDqeXTpk1jwYIFqfkdO3YwdepUVqxYwYYNGwBSh4ZGjRrFiy++CMCLL76YWt/e7t27KS4uZtCgQXzwwQf8/vfB9waddtppbNmyhZUrVwKwZ8+eVGjdcMMN3HzzzUyaNInS0tIj/j1FpOcoCLpBeXk5559/PmeeeSZf+9rXDlo/ffp0EokEY8eOZd68eW0OvRyuO++8k1mzZjFhwgQqKipSy7/+9a+zY8cOzjzzTMaNG8fy5csZMmQICxcu5LLLLmPcuHHMnj0bgMsvv5zt27dzxhln8IMf/IBTTz21w9caN24c48ePZ8yYMXz+85/n/PPPB6CgoIDHH3+cuXPnMm7cOKZNm5b6pDBhwgRKSkq47rrrjvh3FJGeZe6e7RoOy8SJE726urrNstdee42xY8dmqSJJt3nzZi666CLWrVtHTk529zP0dyFygJmtcveJHa3TJwLpNg8//DBTpkzh29/+dtZDQEQy1+euGpLsueaaa7jmmmuyXYaIHCbttomIxJyCQEQk5hQEIiIxpyAQEYk5BUGWDBgwAAgut7ziiis6bHPRRRfR/lLZ9u6//3727duXmtew1iJyuBQEWXbCCSewePHiI35++yA4Voe17oyGuxbJvr53+ejv58H7r3TvNo87Cy65p9PV8+bNY8SIEXz5y18GSA3z/KUvfYmZM2eyY8cOmpubufvuu5k5c2ab527cuJFPfvKTrFmzhv3793Pdddfx8ssvM2bMGPbv359qd+ONNx40HPT8+fPZvHkzH/3oR6moqGD58uWpYa0rKiq47777ePDBB4Fg6Idbb72VjRs3arhrEWmj7wVBFsyePZtbb701FQSLFi3iqaeeorCwkCeeeIKSkhK2bdvG1KlTmTFjRqffp/ujH/2IoqIiXnvtNVavXs25556bWtfRcNA333wz9913H8uXL28z3ATAqlWr+NnPfsYLL7yAuzNlyhQ+8pGPUFpaquGuRaSNvhcEXey5R2X8+PFs3bqVzZs3U1tbS2lpKSNGjKC5uZnbb7+dFStWkJOTw3vvvccHH3zAcccd1+F2VqxYwc033wzA2Wefzdlnn51a19Fw0Onr2/vTn/7EZz7zmdRoopdddhl//OMfmTFjhoa7FpE2+l4QZMmsWbNYvHgx77//fmpwt0ceeYTa2lpWrVpFfn4+o0aNOqJho1uHg165ciWlpaVce+21R7SdVu2Hu04/BNVq7ty5fOUrX2HGjBk888wz3HnnnYf9Ooc73HWmv1/74a5XrVp12LWJyAE6WdxNZs+ezWOPPcbixYuZNWsWEAwZPXToUPLz81m+fDnvvPNOl9u48MIL+dWvfgXAmjVrWL16NdD5cNDQ+RDYH/7wh/nNb37Dvn372Lt3L0888QQf/vCHM/59NNy1SHwoCLrJGWecwcbGCyoAAAXcSURBVJ49exg+fDjHH388AFdddRXV1dWcddZZPPzww4wZM6bLbdx4443U19czduxY7rjjDiZMmAB0Phw0wJw5c5g+fTof/ehH22zr3HPP5dprr2Xy5MlMmTKFG264gfHjx2f8+2i4a5H40DDU0itlMty1/i5EDtAw1NKnaLhrke6lk8XS62i4a5Hu1Wd2p3rbIS6Jlv4eRDLXJ4KgsLCQuro6/eMXIAiBuro6CgsLs12KSK/QJw4NVVZWUlNTQ21tbbZLkWNEYWEhlZWV2S5DpFfoE0GQn5+fuqtVREQOT6SHhsxsupm9bmbrzWxeB+v7mdnj4foXzGxUlPWIiMjBIgsCM8sFFgCXAKcDV5rZ6e2aXQ/scPdTgH8B/imqekREpGNRfiKYDKx397fdvQl4DJjZrs1MoHX8gsXAxdbZ0JwiIhKJKM8RDAc2pc3XAFM6a+PuCTPbBZQD29IbmdkcYE44W29mrx9hTRXttx1zej/a0vtxgN6LtvrC+zGysxW94mSxuy8EFh7tdsysurNbrONI70dbej8O0HvRVl9/P6I8NPQeMCJtvjJc1mEbM8sDBgF1EdYkIiLtRBkEK4HRZlZlZgXA54Al7dosAf4mnL4C+G/XXWEiIj0qskND4TH/m4CngFzgQXd/1czuAqrdfQnwU+AXZrYe2E4QFlE66sNLfYzej7b0fhyg96KtPv1+9LphqEVEpHv1ibGGRETkyCkIRERiLjZBcKjhLuLCzEaY2XIzW2tmr5rZLdmu6VhgZrlm9pKZ/S7btWSbmQ02s8Vmts7MXjOzD2W7pmwxs78L/52sMbNHzaxPDmkbiyDIcLiLuEgAt7n76cBU4Msxfi/S3QK8lu0ijhHfA/7g7mOAccT0fTGz4cDNwER3P5PgopeoL2jJilgEAZkNdxEL7r7F3V8Mp/cQ/CMfnt2qssvMKoFLgZ9ku5ZsM7NBwIUEV/Th7k3uvjO7VWVVHtA/vM+pCNic5XoiEZcg6Gi4i1h3fgDhaK/jgReyW0nW3Q/8PZDMdiHHgCqgFvhZeKjsJ2ZWnO2issHd3wO+C7wLbAF2uft/ZreqaMQlCKQdMxsA/Bq41d13Z7uebDGzTwJb3X1Vtms5RuQB5wI/cvfxwF4glufUzKyU4MhBFXACUGxmV2e3qmjEJQgyGe4iNswsnyAEHnH3f892PVl2PjDDzDYSHDL8mJn9MrslZVUNUOPurZ8SFxMEQxx9HNjg7rXu3gz8O3BelmuKRFyCIJPhLmIhHOb7p8Br7n5ftuvJNnf/3+5e6e6jCP4u/tvd++ReXybc/X1gk5mdFi66GFibxZKy6V1gqpkVhf9uLqaPnjjvFaOPHq3OhrvIclnZcj7wBeAVM/tLuOx2d1+axZrk2DIXeCTcaXobuC7L9WSFu79gZouBFwmutnuJPjrUhIaYEBGJubgcGhIRkU4oCEREYk5BICIScwoCEZGYUxCIiMScgkCkHTNrMbO/pP102521ZjbKzNZ01/ZEukMs7iMQOUz73f2cbBch0lP0iUAkQ2a20czuNbNXzOzPZnZKuHyUmf23ma02s2VmdmK4fJiZPWFmL4c/rcMT5JrZA+E49/9pZv2z9kuJoCAQ6Uj/doeGZqet2+XuZwE/IBi1FOD7wEPufjbwCDA/XD4f+H/uPo5gvJ7Wu9lHAwvc/QxgJ3B5xL+PSJd0Z7FIO2ZW7+4DOli+EfiYu78dDtz3vruXm9k24Hh3bw6Xb3H3CjOrBSrdvTFtG6OA/3L30eH8PwD57n539L+ZSMf0iUDk8Hgn04ejMW26BZ2rkyxTEIgcntlpj8+F089y4CsMrwL+GE4vA26E1HciD+qpIkUOh/ZERA7WP21kVgi+v7f1EtJSM1tNsFd/ZbhsLsE3en2N4Nu9WkfrvAVYaGbXE+z530jwTVcixxSdIxDJUHiOYKK7b8t2LSLdSYeGRERiTp8IRERiTp8IRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5v4/7bshX2fAgf0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNMZW6VCBtGN",
    "outputId": "196905fc-b88c-4142-cb51-483f44f5a4ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8180000000000001"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKE8sBz6B6F7"
   },
   "source": [
    "Test Accuracy#\n",
    "\n",
    "0.818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "36xRK7P7BwJq"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "              )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        review_texts.extend(texts)\n",
    "        predictions.extend(preds)\n",
    "        prediction_probs.extend(outputs)\n",
    "        real_values.extend(targets)\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcmIvdosB47L",
    "outputId": "7a618590-98a7-4161-d9b1-2677eab77aa7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CbROomBCBOt",
    "outputId": "fb4d419b-14f7-4070-dc09-a4df9f13bb72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.79      0.77       230\n",
      "     neutral       0.35      0.31      0.33       104\n",
      "    positive       0.90      0.91      0.90       666\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.67      0.67      0.67      1000\n",
      "weighted avg       0.81      0.82      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "lQAmqz_GCFSJ",
    "outputId": "db53f847-d49d-44f6-b159-c985be16dd15"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment')\n",
    "  \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "nFHXr0woCHf5"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "  'class_names': class_names,\n",
    "  'values': y_pred_probs[idx]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztdsM8vJCKLa",
    "outputId": "7ab22a0f-c0f0-4100-b8b9-b98afdb30a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaging was ok ok but shoes are as expected\n",
      "\n",
      "True sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(wrap(review_text)))\n",
    "print()\n",
    "print(f'True sentiment: {class_names[true_sentiment]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "Yhbgdk8NCMOm",
    "outputId": "15ed5188-6a9a-496e-dbb4-c6014ad7637b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
    "plt.ylabel('sentiment')\n",
    "plt.xlabel('probability')\n",
    "plt.xlim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjMvNxUtCOGI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BERT for sentiment analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "207618094cc944c998f49b741f7d46a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dddf1830be54c5aaf4680f0d8edd3b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_474e8e10b77e4fee8bcc7daafdc7f09f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7fc8c787e07d4823a2767e9797f6917f",
      "value": " 213k/213k [00:00&lt;00:00, 185kB/s]"
     }
    },
    "3151a61c5f824d1c987869a39a8d3d55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "474e8e10b77e4fee8bcc7daafdc7f09f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e245282cee34bb09905c93f84c45cbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccc45c81f2e94ddcb8b03a51b06ef8dc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_97ebdf19fdc0458aaa20efa0f8741837",
      "value": "Downloading: 100%"
     }
    },
    "62831f1fe5a04cfb980eb288bdbd198f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fc8c787e07d4823a2767e9797f6917f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "877c8a631bd54595a66165a7c31137db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_207618094cc944c998f49b741f7d46a5",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62831f1fe5a04cfb980eb288bdbd198f",
      "value": 213450
     }
    },
    "97ebdf19fdc0458aaa20efa0f8741837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c31f08a2e1b141d8ad890375934036a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e245282cee34bb09905c93f84c45cbb",
       "IPY_MODEL_877c8a631bd54595a66165a7c31137db",
       "IPY_MODEL_2dddf1830be54c5aaf4680f0d8edd3b8"
      ],
      "layout": "IPY_MODEL_3151a61c5f824d1c987869a39a8d3d55"
     }
    },
    "ccc45c81f2e94ddcb8b03a51b06ef8dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
